<!doctype html>
<html>

<head>

  <title>
    
      Multiclass Classification | Mahesh Kumar
    
  </title>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/syntax.css">
  <!-- Use Atom -->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/rss-feed.xml" title="Mahesh Kumar" />
  <!-- Use RSS-2.0 -->
  <!--<link href="/rss-feed.xml" type="application/rss+xml" rel="alternate" title="Mahesh Kumar | "/>
  //-->

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Quattrocento+Sans">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82492711-1', 'auto');
  ga('send', 'pageview');
</script>


  <!-- Use Jekyll SEO plugin -->
  <!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Multiclass Classification | Mahesh Kumar</title>
<meta name="generator" content="Jekyll v3.6.2" />
<meta property="og:title" content="Multiclass Classification" />
<meta name="author" content="Mahesh Kumar K" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="According to Wikipedia:" />
<meta property="og:description" content="According to Wikipedia:" />
<link rel="canonical" href="http://localhost:4000/article/multiclass-classification.html" />
<meta property="og:url" content="http://localhost:4000/article/multiclass-classification.html" />
<meta property="og:site_name" content="Mahesh Kumar" />
<meta property="og:image" content="http://localhost:4000/multiclass-6.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-03-28T16:47:14+05:30" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Multiclass Classification","dateModified":"2017-03-28T16:47:14+05:30","datePublished":"2017-03-28T16:47:14+05:30","image":"http://localhost:4000/multiclass-6.jpeg","url":"http://localhost:4000/article/multiclass-classification.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/article/multiclass-classification.html"},"author":{"@type":"Person","name":"Mahesh Kumar K"},"description":"According to Wikipedia:","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


</head>


<body>

  <div class="container">
    <header class="masthead">
  <h3 class="masthead-title">
    <a href="/">Mahesh Kumar</a>
    <!-- <small class="masthead-subtitle"></small> -->
    <div class="menu">
  <nav class="menu-content">
    
      <a href="/menu/about.html">About</a>
    
  </nav>
  <nav class="social-icons">
    
  
  
    <a href="https://www.github.com/maheshkkumar" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  

  
  
    <a href="https://twitter.com/maheshkumark_" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  

  
  
    <a href="https://www.linkedin.com/in/maheshkumark" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
  

  
  
    <a href="mailto:maheshk2194@gmail.com" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  

  
  
    <a href="/rss-feed.xml"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
  

  </nav>
</div>

  </h3>
</header>


    <div class="post-container">
      <h1>
  Multiclass Classification
</h1>


  <img src="/assets/img/multiclass-6.jpeg">


<p>According to Wikipedia:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In machine learning, multiclass or multinomial classification is the problem of classifying instances into one of the more than two classes (classifying instances into one of the two classes is called binary classification).
</code></pre></div></div>

<p>For instance, you are given an Apple product and you are told to classify it into one of the following:</p>

<ol>
  <li>Apple Watch</li>
  <li>Apple TV</li>
  <li>iMac</li>
  <li>iPad</li>
  <li>iPhone</li>
  <li>iPod</li>
  <li>MacBook</li>
</ol>

<p>So, this is a type of multiclass classification problem.</p>

<p>In this post, let’s learn how to approach a multiclass classification problem. We will be using a dataset that has 5 categories to which we should classify the given data.</p>

<h4 id="tldr">TL;DR</h4>
<p>Check out the <a href="https://github.com/maheshkkumar/multiclass-classification">code</a> for this problem.</p>

<p>Let’s import all the necessary libraries.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># imports</span>

<span class="c"># numpy, lambda, matplotlib</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c"># scikit-learn</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">ShuffleSplit</span>
<span class="kn">from</span> <span class="nn">sklearn.learning_curve</span> <span class="kn">import</span> <span class="n">learning_curve</span>

<span class="c"># I will be explaining the purpose of each library as we approach the problem.</span>
</code></pre></div></div>

<p>Let’s read the input data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># train and test data</span>
<span class="c"># here we will be using pandas library to read the input data in the form of CSV</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'train.csv'</span><span class="p">)</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'test.csv'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># let's see what's inside the training data</span>
<span class="c"># pandas is a good library to play with data, also you can clean the data.</span>

<span class="n">train</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>feature0</th>
      <th>feature1</th>
      <th>...</th>
      <th>feature247</th>
      <th>feature248</th>
      <th>feature249</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>25.208249</td>
      <td>-16.745748</td>
      <td>...</td>
      <td>-4.938090</td>
      <td>130.068955</td>
      <td>-8.231081</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>-86.931450</td>
      <td>0.428227</td>
      <td>...</td>
      <td>-6.389429</td>
      <td>-26.164277</td>
      <td>-4.909740</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>42.160934</td>
      <td>7.857013</td>
      <td>...</td>
      <td>8.034066</td>
      <td>120.510705</td>
      <td>7.754377</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 252 columns</p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># let's check the shape of the dataset</span>

<span class="n">train</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(7200, 252)
</code></pre></div></div>

<p>Looks like the training dataset contains 7200 rows and 252 columns.</p>

<p>Also, there are 250 <em>features</em>, an <em>id</em> column and a <em>label</em> column.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># let's check the information about the training and test dataset</span>

<span class="n">train</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"-"</span><span class="p">)</span><span class="o">*</span><span class="mi">40</span>
<span class="n">test</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 7200 entries, 0 to 7199
Columns: 252 entries, id to label
dtypes: float64(250), int64(2)
memory usage: 13.8 MB
----------------------------------------
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 4800 entries, 0 to 4799
Columns: 251 entries, id to feature249
dtypes: float64(250), int64(1)
memory usage: 9.2 MB
</code></pre></div></div>

<p>Now, let’s clean the training dataset for any null values</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># checking for null values</span>

<span class="n">train</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0L
</code></pre></div></div>

<p>Looks like there are no null values in the dataset</p>

<p>Now, let’s do feature selection, where we use the features which we think are important for classification later on on new data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># feature selection</span>

<span class="c"># seperate labels from training set</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s">'label'</span><span class="p">]</span>

<span class="c"># drop or remove id and label columns from training set</span>
<span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'id'</span><span class="p">,</span> <span class="s">'label'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s see how the training set looks after some trimming.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># training set after removing id and label column</span>

<span class="n">train</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature0</th>
      <th>feature1</th>
      <th>feature2</th>
      <th>...</th>
      <th>feature246</th>
      <th>feature247</th>
      <th>feature248</th>
      <th>feature249</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>25.208249</td>
      <td>-16.745748</td>
      <td>50.869944</td>
      <td>...</td>
      <td>3.682115</td>
      <td>-4.938090</td>
      <td>130.068955</td>
      <td>-8.231081</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-86.931450</td>
      <td>0.428227</td>
      <td>2.874836</td>
      <td>...</td>
      <td>6.484784</td>
      <td>-6.389429</td>
      <td>-26.164277</td>
      <td>-4.909740</td>
    </tr>
    <tr>
      <th>2</th>
      <td>42.160934</td>
      <td>7.857013</td>
      <td>151.612757</td>
      <td>...</td>
      <td>-1.971843</td>
      <td>8.034066</td>
      <td>120.510705</td>
      <td>7.754377</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 250 columns</p>
</div>

<p>Let’s do the same for test data, remove id column</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># what test dataset looks like</span>

<span class="n">test</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>feature0</th>
      <th>feature1</th>
      <th>feature2</th>
      <th>...</th>
      <th>feature247</th>
      <th>feature248</th>
      <th>feature249</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7200</td>
      <td>-76.795547</td>
      <td>-12.319618</td>
      <td>-109.961631</td>
      <td>...</td>
      <td>2.619267</td>
      <td>-14.869630</td>
      <td>-4.718944</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7201</td>
      <td>15.860563</td>
      <td>-5.344301</td>
      <td>74.481876</td>
      <td>...</td>
      <td>-16.322332</td>
      <td>-54.813692</td>
      <td>14.470087</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7202</td>
      <td>21.243900</td>
      <td>-7.539082</td>
      <td>-13.128054</td>
      <td>...</td>
      <td>-3.615364</td>
      <td>160.603909</td>
      <td>-11.330304</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 251 columns</p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># dropping id column and storing it for later use</span>

<span class="n">test_ids</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s">'id'</span><span class="p">]</span>
<span class="n">test</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'id'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Now, we have segregated the data according to our need.</p>

<p>Let’s do some visualizations on the labels.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># count of values under each category</span>

<span class="n">label_count</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">sort_index</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span> <span class="n">label_count</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0    1419
1    1426
2    1485
3    1410
4    1460
Name: label, dtype: int64
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># visualizing the training labels</span>

<span class="n">label_count</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">'bar'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'skyblue'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"categories"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Count of data"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Count of data in each category"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/img/multiclass-1.png" alt="Count in each category" /></p>

<p>From the visualization, it’s clear that data is spread evenly across all the 5 categories.</p>

<p>Next, let’s split the training set into training set and validation set.</p>

<p>For splitting the dataset, we will be making use of train_test_split from sklearn.</p>

<p>Training Set = 80%
Validation Set = 20%</p>

<p>The reason we use <strong>random_state</strong> is that the splitting of the data must not be random, as it might ruin the consistency.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># splitting of the dataset</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">21</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># check out the shape of each of the above train and test datasets</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Shape of X_train: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Shape of y_train: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"-"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Shape of X_test: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Shape of y_test: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Shape of X_train: (5760, 250)
Shape of y_train: (5760L,)
--------------------------------------------------
Shape of X_test: (1440, 250)
Shape of y_test: (1440L,)
</code></pre></div></div>

<p>Looks like we have got 80% and 20% split</p>

<p>Next, let’s visualize the range of values in training set</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># visualizing the range of values in training set</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">X_train</span><span class="p">[</span><span class="s">"feature{}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Range of datapoints"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Feature0 to Feature249"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/img/multiclass-2.png" alt="Range of training data without standardization" /></p>

<p>Looks like the datapoints range from <strong>-300</strong> to <strong>300</strong>. We shall bring this range to a single digit.</p>

<p>In-order to standardize the data, let’s use the StandardScaler from sklearn.</p>

<p>StandardScaler brings down the range of values a single digit, where the new values are <strong>0</strong>. 
StandardScaler applies a normal distribution, with <strong>mean = 0</strong> and <strong>standard deviation = 1</strong>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># standardize the data</span>
<span class="n">std_scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c"># transform train and test set using standardization</span>
<span class="n">X_train_std</span> <span class="o">=</span> <span class="n">std_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_std</span> <span class="o">=</span> <span class="n">std_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s visualize the range of datapoints in training set</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># range of values in training dataset after standardization</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Range of the data points'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/img/multiclass-3.png" alt="Range of data after standardization" /></p>

<p>Standardization worked!</p>

<p>The datapoints are now distributed between <strong>-4</strong> and <strong>4</strong>.</p>

<p>Next, let’s convert the <strong>y_train</strong> and <strong>y_test</strong> pandas series to numpy arrays.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># converting from pandas series to numpy array</span>

<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'float64'</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'float64'</span><span class="p">)</span>
</code></pre></div></div>

<p>So, all the major cleaning process is done, next we need to choose a classification algorithm to train it.</p>

<p>But, before that we need to know what are the appropriate hyper parameters that our learning algorithm <strong>KNeighborsClassifier</strong> works efficiently.</p>

<p>In-order to choose the appropriate hyper parameters, let’s use <strong>GridSearchCV</strong> from <strong>model_selection</strong>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># an instane of KNNeighborsClassifier</span>
<span class="n">estimator</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>

<span class="c"># cross-validation using ShuffleSplit</span>
<span class="c"># we do this so the GridSearchCV will train and test the dataset to find the appropriate hyperparameters</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">ShuffleSplit</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">)),</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c"># instance of GridSearchCV</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">13</span><span class="p">,</span><span class="mi">14</span><span class="p">,</span><span class="mi">15</span><span class="p">]))</span>

<span class="c"># let's fit the classifier with the training set</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GridSearchCV(cv=ShuffleSplit(5760, n_iter=10, test_size=0.2, random_state=0),
       error_score='raise',
       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights='uniform'),
       fit_params={}, iid=True, n_jobs=1,
       param_grid={'n_neighbors': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]},
       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,
       scoring=None, verbose=0)
</code></pre></div></div>

<p>That took a while to figure out the appropriate <strong>n_neighbors</strong> value</p>

<p>Let’s check the value of <strong>n_neighbors</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span> <span class="n">classifier</span><span class="o">.</span><span class="n">best_params_</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'n_neighbors': 15}
</code></pre></div></div>

<p>Looks like assigning <strong>n_neighbors</strong> to <strong>15</strong> is a good choice.</p>

<p>Let’s train the KNeighborsClassifier with the best params value</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># instance of KNeighborsClassifier</span>

<span class="n">estimator</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># fit the training data</span>

<span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=15, p=2,
           weights='uniform')
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># testing on validation set</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_std</span><span class="p">)</span>
</code></pre></div></div>

<p>Now, let’s print <strong>accuracy_score</strong>, <strong>confusion_matrix</strong> and <strong>classification_report</strong> from <strong>sklearn.metrics</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># printing out some metrics</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"-"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Confusion Matrix: </span><span class="se">\n</span><span class="s">{}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"-"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Classification Report: </span><span class="se">\n</span><span class="s">{}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy: 0.970833333333
--------------------------------------------------
Confusion Matrix: 
[[274   2   0   1   0]
 [  2 250   3   1   3]
 [  1   3 280   3   0]
 [  4   6   3 294   3]
 [  3   3   1   0 300]]
--------------------------------------------------
Classification Report: 
             precision    recall  f1-score   support

        0.0       0.96      0.99      0.98       277
        1.0       0.95      0.97      0.96       259
        2.0       0.98      0.98      0.98       287
        3.0       0.98      0.95      0.97       310
        4.0       0.98      0.98      0.98       307

avg / total       0.97      0.97      0.97      1440
</code></pre></div></div>

<p>All the above scores are fine, let’s have a look at the learning curve.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">leaning_curve_plot</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Learning Curve"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Traning Example"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Score"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
    
    <span class="c"># compute mean and standard deviation</span>
    <span class="n">train_sizes</span><span class="p">,</span> <span class="n">train_scores</span><span class="p">,</span> <span class="n">test_scores</span> <span class="o">=</span> <span class="n">learning_curve</span><span class="p">(</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">train_sizes</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">train_scores_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">train_scores_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">train_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_scores_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_scores_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">test_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c"># fill_between </span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">train_scores_mean</span> <span class="o">-</span> <span class="n">train_scores_std</span><span class="p">,</span> <span class="n">train_scores_mean</span> <span class="o">+</span> <span class="n">train_scores_std</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">test_scores_mean</span> <span class="o">-</span> <span class="n">test_scores_std</span><span class="p">,</span> <span class="n">test_scores_mean</span> <span class="o">+</span> <span class="n">test_scores_std</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'g'</span><span class="p">)</span>
    
    <span class="c"># plotting </span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">train_scores_mean</span><span class="p">,</span> <span class="s">'o-'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Trainig score"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">test_scores_mean</span><span class="p">,</span> <span class="s">'o-'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Cross Validation score"</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">"best"</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">plt</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span> <span class="o">=</span> <span class="n">leaning_curve_plot</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X_train_std</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/img/multiclass-4.png" alt="Learning Curve" /></p>

<p>Looks like the estimator’s score goes high when the dataset size increases.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Time to classify the test set</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">std_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># saving the output to a csv</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'id'</span><span class="p">:</span> <span class="n">test_ids</span><span class="p">,</span> <span class="s">'label'</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">})</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># printing output</span>
<span class="n">output</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div></div>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7200</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7201</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7202</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7203</td>
      <td>4.0</td>
    </tr>
  </tbody>
</table>
</div>

<p>Now, let’s look at the decision boundary for the given dataset</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># reducing the dimensionality from 15 to 2</span>
<span class="n">X_train_embedded</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">)</span>
<span class="k">print</span> <span class="n">X_train_embedded</span><span class="o">.</span><span class="n">shape</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">)</span>

<span class="c"># creating meshgrid</span>
<span class="n">resolution</span> <span class="o">=</span> <span class="mi">150</span> <span class="c"># 150x150 background pixels</span>
<span class="n">X2d_xmin</span><span class="p">,</span> <span class="n">X2d_xmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">min</span><span class="p">(</span><span class="n">X_train_embedded</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">X_train_embedded</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">X2d_ymin</span><span class="p">,</span> <span class="n">X2d_ymax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">min</span><span class="p">(</span><span class="n">X_train_embedded</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">X_train_embedded</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X2d_xmin</span><span class="p">,</span> <span class="n">X2d_xmax</span><span class="p">,</span> <span class="n">resolution</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X2d_ymin</span><span class="p">,</span> <span class="n">X2d_ymax</span><span class="p">,</span> <span class="n">resolution</span><span class="p">))</span>

<span class="c"># approximate Voronoi tesselation on resolution x resolution grid using 1-NN</span>
<span class="n">background_model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_embedded</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">)</span> 
<span class="n">voronoiBackground</span> <span class="o">=</span> <span class="n">background_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="n">voronoiBackground</span> <span class="o">=</span> <span class="n">voronoiBackground</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">resolution</span><span class="p">,</span> <span class="n">resolution</span><span class="p">))</span>

<span class="c">#plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">voronoiBackground</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train_embedded</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_train_embedded</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(5760L, 2L)
</code></pre></div></div>

<p><img src="/assets/img/multiclass-5.png" alt="Decision Boundary" /></p>

<p>Here, we see a clear classification of the datapoints into one of the five categories.</p>

<p>Hope you have understood the process of solving a multiclass classification problem.</p>


<span class="post-date">
  Written on
  
  March
  28th,
  2017
  by
  
    Mahesh Kumar K
  
</span>

<div class="post-date">Feel free to share!</div>
  <div class="sharing-icons">
    <a href="https://twitter.com/intent/tweet?text=Multiclass Classification&amp;url=/article/multiclass-classification.html" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
    <a href="https://www.facebook.com/sharer/sharer.php?u=/article/multiclass-classification.html&amp;title=Multiclass Classification" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
    <a href="https://plus.google.com/share?url=/article/multiclass-classification.html" target="_blank"><i class="fa fa-google-plus" aria-hidden="true"></i></a>
  </div>
</div>


<div class="related">
  <h1 >You may also enjoy:</h1>
  
  <ul class="related-posts">
    
      
        
          <li>
            <h3>
              <a href="/article/a-guide-to-deploying-machine-learning-models-in-production.html">
                A guide to deploying Machine/Deep Learning model(s) in Production
                <!--<img src="http://localhost:4000/images/">-->
                <!--<small>June 25, 2018</small>-->
              </a>
            </h3>
          </li>
          
        
      
        
        
      
        
          <li>
            <h3>
              <a href="/article/anatomy-of-natural-language-processing.html">
                Anatomy of Natural Language Processing
                <!--<img src="http://localhost:4000/images/">-->
                <!--<small>March 13, 2017</small>-->
              </a>
            </h3>
          </li>
          
        
      
    
  </ul>
</div>



  <section class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname = "www-maheshkumar-xyz";
    var disqus_identifier = "/article/multiclass-classification.html";
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



    </div>

    <footer class="footer">
  
  
  
    <a href="https://www.github.com/maheshkkumar" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  

  
  
    <a href="https://twitter.com/maheshkumark_" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  

  
  
    <a href="https://www.linkedin.com/in/maheshkumark" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
  

  
  
    <a href="mailto:maheshk2194@gmail.com" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  

  
  
    <a href="/rss-feed.xml"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
  

  <!-- <div class="post-date"><a href="/menu/about.html">Mahesh Kumar |  by Mahesh Kumar</a></div> -->
  <div class="post-date">Theme Courtesy -<a href="https://github.com/LeNPaul/Lagrange">Lagrange</a><i class="fas fa-thumbs-up"></i></div>
</footer>

  </div>

</body>
</html>
